{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9-1 : XOR을 위한 텐스플로우 딥넷트웍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR data set\n",
    "\n",
    "| A | B | X |\n",
    "| - | - | - |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "#### Boolean Expression\n",
    "$$\n",
    "X = A \\oplus B\n",
    "$$\n",
    "\n",
    "#### Logic Diagram Symbol\n",
    "\n",
    "<img src=\"./10.png\" width=\"400\" height=\"auto\" alt=\"아직 안만듬\">\n",
    "\n",
    "[[이미지 출처]](https://mathphysics.tistory.com/579)\n",
    "\n",
    "간단한 데이터셋이므로 따로 입력받지않고 **numpy array** 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1],\n",
    "    ],\n",
    "    dtype=np.float32\n",
    ")\n",
    "y_data = np.array(\n",
    "    [\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0],\n",
    "    ],\n",
    "    dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Logistic Regression으로 해결하기\n",
    "**0**인지 **1**인지 예측하는 **Logistic Regression**사용<br/>\n",
    "$$X$$, $$Y$$에 맞게 **Weight**과 **Bias**를 결정해야한다.<br/>\n",
    "**Weigth**을 정할때에는 **크기**가 **중요**하다.<br/>\n",
    "입력값이 $$X_1$$, $$X_2$$로 2개고 출력값이 $$Y$$하나이기 때문에<br/>\n",
    "**Weight**의 크기는 **[2, 1]**의 크기다.<br/>\n",
    "**Bias**는 항상 **출력값의 갯수**와 같으므로 **[1]**이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Hypothesis\n",
    "**Sigmoid** 함수 사용<br/>\n",
    "\\begin{align} Sigmoid = \\frac{1}{1+e^{-x}} \\end{align}\n",
    "\n",
    "**행렬 곱셉** 후 **Sigmoid Function**에 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Cost Function\n",
    "**Logistic Regression**의 **Cost Function**은 아래와 같다.<br/>\n",
    "$$\n",
    "C(H(x), y) = \n",
    "-Y * log(H(x)\n",
    "-(1 - Y) * log(1 - H(X))\n",
    "$$\n",
    "\n",
    "구해진 Cost를 가지고 **경사하강법**을 사용해 학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Training\n",
    "위에 작성한 **Hypothesis**와 **Cost Function**을 사용해 학습을 진행<br/>\n",
    "`cast`함수를 사용해 **Hypothesis**의 값이 **0.5**보다 클경우 **True**로<br/>\n",
    "**0.5**보다 작을경우 **False**로 값을 바꾸어준다.<br/>\n",
    "또한 예측값과 결과값을 비교해 `cast`함수를 사용한 **결과의 평균**을 구해 **정확도**를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.99409974 [[-1.081909 ]\n",
      " [ 2.3671274]]\n",
      "1000 0.75658506 [[-0.85761964]\n",
      " [ 1.1723609 ]]\n",
      "2000 0.7131047 [[-0.46316972]\n",
      " [ 0.65583754]]\n",
      "3000 0.699124 [[-0.23715353]\n",
      " [ 0.36725613]]\n",
      "4000 0.6949206 [[-0.11818436]\n",
      " [ 0.20615283]]\n",
      "5000 0.69367814 [[-0.05713516]\n",
      " [ 0.11657296]]\n",
      "6000 0.69330937 [[-0.02641575]\n",
      " [ 0.06656651]]\n",
      "7000 0.6931982 [[-0.01132161]\n",
      " [ 0.03844175]]\n",
      "8000 0.6931639 [[-0.00415682]\n",
      " [ 0.02247488]]\n",
      "9000 0.69315296 [[-0.00093976]\n",
      " [ 0.01331245]]\n",
      "10000 0.6931492 [[0.00036489]\n",
      " [0.00799207]]\n",
      "\n",
      "Hypothesis:\n",
      "[[0.4987609 ]\n",
      " [0.50075895]\n",
      " [0.49885216]\n",
      " [0.5008502 ]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy:\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run(\n",
    "            [train, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "            \n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 정확하고 문제가 없음에도 불구하고 정확도가 높지않다.<br/>\n",
    "정확도를 올리기 위해서 **Neural Network**를 사용하면 된다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Using Neural Network\n",
    "여러개의 Layer를 사용하는 **NN**을 사용<br/>\n",
    "layer1의 결과물을 hypothesis에 넣어 한번더 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 **weight의 크기**를 잘 정해주어야 한다.<br/>\n",
    "\n",
    "layer1의 입력값은 $$X_1$$, $$X_2$$로 **2개**이고 출력값의 갯수는<br/>\n",
    "임의로 결정하면 되므로 $$W_1$$의 크기는 **[2, 2]**로 정했다.<br/>\n",
    "출력값의 갯수를 2개로 결정했으니 **bias**의 크기는 **[2]**가 된다.<br/>\n",
    "\n",
    "layer1을 입력으로 받는 다른 layer의 입력값의 개수는<br/>\n",
    "**layer1의 입력값**인 **2개**로 같고 출력값은<br/>\n",
    "$$ \\bar{Y} $$이므로 **1개**이기 때문에 **weight**의 크기는 **[2. 1]**이다.<br/>\n",
    "**bias**의 크기는 출력값이 **1개**이기 때문에 **[1]**이다.<br/>\n",
    "\n",
    "따라서 결정된 Layer들의 **Weigth**과 **Bias**의 크기는 아래와 같다.<br/>\n",
    "\n",
    "|        | Weight | Bias |\n",
    "| ------ | ------ | ---- |\n",
    "| Layer1 | [2, 2] | [2]  |\n",
    "| Layer2 | [2, 1] | [1]  | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Training\n",
    "2개의 Layer를 겹친 **Neural Network**를 이용해 한번 더 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.82048273\n",
      "1000 0.67851186\n",
      "2000 0.606001\n",
      "3000 0.4774453\n",
      "4000 0.1442676\n",
      "5000 0.06338048\n",
      "6000 0.039486427\n",
      "7000 0.028429154\n",
      "8000 0.022119436\n",
      "9000 0.01806068\n",
      "10000 0.015238974\n",
      "\n",
      "Hypothesis:\n",
      "[[0.01475588]\n",
      " [0.9810414 ]\n",
      " [0.985857  ]\n",
      " [0.01261569]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run(\n",
    "            [train, cost], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 **Hypothesis**와 **Cost Function**을 사용하였으나,<br/>\n",
    "2개의 Layer를 사용한 것만으로 **모든 값을 예측**하는데 성공하였다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### 전체적인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0340692\n",
      "1000 0.68017256\n",
      "2000 0.5653977\n",
      "3000 0.24299939\n",
      "4000 0.09485774\n",
      "5000 0.053028427\n",
      "6000 0.035822317\n",
      "7000 0.026766824\n",
      "8000 0.021257266\n",
      "9000 0.017577857\n",
      "10000 0.014957345\n",
      "\n",
      "Hypothesis:\n",
      "[[0.01222286]\n",
      " [0.98380554]\n",
      " [0.9837938 ]\n",
      " [0.01474641]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run(\n",
    "            [train, cost], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Deep Neural Network로 XOR 해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.786564\n",
      "1000 0.21167177\n",
      "2000 0.030031722\n",
      "3000 0.013181003\n",
      "4000 0.008059783\n",
      "5000 0.005704046\n",
      "6000 0.0043751714\n",
      "7000 0.0035298648\n",
      "8000 0.0029479803\n",
      "9000 0.0025245028\n",
      "10000 0.0022032897\n",
      "\n",
      "Hypothesis:  [[0.00249692]\n",
      " [0.99716216]\n",
      " [0.998467  ]\n",
      " [0.00193409]] \n",
      "Predicted:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run(\n",
    "            [train, cost], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHypothesis: \", h, \"\\nPredicted: \", p, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**의 값을 보면 0이 되어야하는 값은<br/>\n",
    "0에 더 가까워졌고 1이 되어야하는 값은 1에 더 가까워졌다.<br/>\n",
    "여러개의 Layer를 사용해 **Deep**하게 학습한 결과<br/>\n",
    "**정확도**가 더 **높아**지게 되었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
