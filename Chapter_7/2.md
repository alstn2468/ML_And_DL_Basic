
## Cost 함수 소개

### Sigmoid?
`0`에서 `1`사이의 값을 필요로 하다.

\begin{align}
    WX = y 
    \begin{bmatrix} 
        2.0 \rightarrow p = 0.7 \\ 
        1.0 \rightarrow p = 0.2 \\ 
        0.1 \rightarrow p = 0.1 \\
    \end{bmatrix}
\end{align}

<br/>

### SoftMax
\begin{align}
    S(y^i) = \frac{e^{y^i}}{\displaystyle \sum_{j}^{} e^{y^j}}
\end{align}


\begin{align}
    y 
    \begin{bmatrix}
        2.0 \\ 
        1.0 \\ 
        0.1 \\
    \end{bmatrix} 
    \rightarrow 
    \begin{bmatrix}
        0.7 \\ 0.2 \\ 0.1 \\
    \end{bmatrix}
    \rightarrow 
    \begin{bmatrix}
        1.0 \\ 0.0 \\ 0.0
    \end{bmatrix}
\end{align}

**SoftMax**을 이용하여 수치를 `0`에서 `1`사이의 값으로 변환하고<br/>
**One Hot Encoding**을 이용하여 마지막 확률을 예측

<br/>

### Cost Function
**Cross - Entropy**를 사용
\begin{align}
    D(S, L) = - \displaystyle \sum_{i}^{} L_i log(S_i)
\end{align}

**Logistic cost**와 **Cross entropy**는 결국 같은 식
\begin{align}
    C(H(x), y) == D(S, L)
\end{align}

**Cost Function**
\begin{align}
    Loss = \frac{1}{N} \displaystyle \sum_{i}^{} D(S(Wx_i+b), L_i)
\end{align}

**Cost**를 계산하며 **Gradient descent**알고리즘을 사용해 최적화된 값을 찾는다.
