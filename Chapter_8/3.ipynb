{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lab 07-1: training/test dataset, learning rate, normalization (new)\n",
    "\n",
    "### Training and Test data sets\n",
    "**Data Set**을 **Training Set**과 **Test Set**으로 나누어 진행<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "x_data = [\n",
    "    [1, 2, 1],\n",
    "    [1, 3, 2],\n",
    "    [1, 3, 4],\n",
    "    [1, 5, 5],\n",
    "    [1, 7, 5],\n",
    "    [1, 2, 5],\n",
    "    [1, 6, 6],\n",
    "    [1, 7, 7],\n",
    "]\n",
    "y_data = [\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "]\n",
    "\n",
    "# Test Set\n",
    "x_test = [\n",
    "    [2, 1, 1],\n",
    "    [3, 1, 2],\n",
    "    [3, 3, 4],\n",
    "]\n",
    "y_test = [\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 상황에서 `placeholder`가 유용하다.<br/>\n",
    "`placeholder`를 이용해서 어떠한 값이 들어올 때<br/>\n",
    "**Traiining Set**을 `placeholder`에 넣어서 학습시키고<br/>\n",
    "**Test Set**을 `placeholder`에 넣어서 테스트를 진행하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.1074553 [[-1.4293944  -0.7224384   2.848082  ]\n",
      " [ 1.1112422  -0.3999512  -0.71154207]\n",
      " [-0.15248409  0.17528887  0.5048751 ]]\n",
      "20 0.7546797 [[-1.5631142  -0.5818348   2.841198  ]\n",
      " [ 0.72321707 -0.0059972  -0.71747077]\n",
      " [-0.10348553  0.66835874 -0.03719316]]\n",
      "40 0.6795351 [[-1.6549793  -0.5410513   2.8922794 ]\n",
      " [ 0.56363654 -0.01940873 -0.54447865]\n",
      " [ 0.08512951  0.65142184 -0.20887122]]\n",
      "60 0.63409156 [[-1.7386525  -0.5033351   2.9382365 ]\n",
      " [ 0.4421414  -0.01824449 -0.42414775]\n",
      " [ 0.2345651   0.6264432  -0.33332816]]\n",
      "80 0.6051625 [[-1.8194332  -0.4668002   2.9824824 ]\n",
      " [ 0.35451818 -0.01411752 -0.34065136]\n",
      " [ 0.34999305  0.60314    -0.42545283]]\n",
      "100 0.58531666 [[-1.8991535  -0.43095058  3.0263534 ]\n",
      " [ 0.29415295 -0.01108143 -0.28332224]\n",
      " [ 0.43837747  0.58400047 -0.49469772]]\n",
      "120 0.57030904 [[-1.9780612  -0.39590144  3.0702124 ]\n",
      " [ 0.25370523 -0.00954942 -0.24440636]\n",
      " [ 0.50700086  0.5684458  -0.54776627]]\n",
      "140 0.55791247 [[-2.0558922  -0.3618978   3.1140394 ]\n",
      " [ 0.22686873 -0.00888192 -0.2182373 ]\n",
      " [ 0.5619558   0.5552294  -0.58950466]]\n",
      "160 0.54703254 [[-2.13234    -0.32911745  3.1577072 ]\n",
      " [ 0.20899145 -0.00845987 -0.20078206]\n",
      " [ 0.6076821   0.5433617  -0.6233629 ]]\n",
      "180 0.5371375 [[-2.2071946  -0.29763928  3.2010844 ]\n",
      " [ 0.19691719 -0.00792996 -0.18923767]\n",
      " [ 0.64718145  0.53225446 -0.65175486]]\n",
      "200 0.5279585 [[-2.2803524  -0.26746872  3.2440712 ]\n",
      " [ 0.18859148 -0.0071582  -0.18168372]\n",
      " [ 0.6824146   0.5216165  -0.6763497 ]]\n",
      "Prediction : [2 2 2]\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tenserflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], \n",
    "                                      feed_dict={X: x_data, Y: y_data})\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "            \n",
    "    # Predict\n",
    "    print(\"Prediction :\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy :\", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 **Accuracy**와 **Predection**의 값은<br/>\n",
    "**Training Set**을 가지고 학습시킨 모델을 가지고<br/>\n",
    "**Test Set**을 예측한 값으로 모델 입장에서 한 번도<br/>\n",
    "**학습하지 않은 데이터**로 예측을한 의미가 있는 결과 값이다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Large Learning Rate\n",
    "Learning Rate가 너무 클경우\n",
    "- **Overshooting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.1369658 [[-0.16136795 -0.052203    1.2202002 ]\n",
      " [ 1.7465985  -3.63817     1.3205297 ]\n",
      " [ 0.01285887 -4.4370604   0.21666038]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction : [0 0 0]\n",
      "Accuracy : 0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tenserflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], \n",
    "                                      feed_dict={X: x_data, Y: y_data})\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "            \n",
    "    # Predict\n",
    "    print(\"Prediction :\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy :\", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate를 1.5로 올린 결과 **Overshooting**이<br/>\n",
    "발생해 학습이 잘 되지않은 모델이 생성되어 예측이 잘 되지않았다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Learning Rate가 너무 작을 경우\n",
    "- Many iterations\n",
    "- Local minima에 빠질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "20 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "40 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "60 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "80 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "100 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "120 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "140 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "160 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "180 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "200 4.4417534 [[ 1.6614894  -2.5569797   1.3669713 ]\n",
      " [-0.43707097  1.3632766   1.4774112 ]\n",
      " [-0.6081834   0.77286756  0.4197207 ]]\n",
      "Prediction : [0 0 2]\n",
      "Accuracy : 0.33333334\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tenserflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], \n",
    "                                      feed_dict={X: x_data, Y: y_data})\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val)\n",
    "            \n",
    "    # Predict\n",
    "    print(\"Prediction :\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy :\", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate를 1e-10으로 낮추었더니,<br>\n",
    "cost가 줄어들지 않고 학습이 이루어지지 않은 결과가 생겼다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Non-normalized inputs\n",
    "아래와 같이 데이터들 간의 차이가 큰 Data Set을 사용하면<br/>\n",
    "한쪽 방향으로 치우쳐진 **왜곡된 그래프**가 그려지게 된다.\n",
    "\n",
    "| x_data (xy[:, 0:-1]) | y_data (xy[:, [-1]) |\n",
    "| - | - |\n",
    "| [828.659973, 833.450012, 908100, 828.349976] | [831.659973] |\n",
    "| [823.02002, 828.070007, 1828100, 821.655029] | [828.070007] |\n",
    "| [816, 820.958984, 1008100, 815.48999] | [819.23999] |\n",
    "| [819.359985, 823, 1188100, 818.469971] | [818.97998] |\n",
    "| [819, 823, 1198100, 816] | [820.450012] |\n",
    "| [811.700012, 815.25, 1098100, 809.780029] | [813.669983] |\n",
    "| [809.51001, 816.659973, 1398100, 804.539978] | [809.559998] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost : 272062040000.0 \n",
      "Prediction\n",
      " [[-367724.94]\n",
      " [-739115.9 ]\n",
      " [-581670.8 ]\n",
      " [-408077.16]\n",
      " [-480746.12]\n",
      " [-484782.1 ]\n",
      " [-444402.88]\n",
      " [-565508.5 ]]\n",
      "40 Cost : nan \n",
      "Prediction\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost : nan \n",
      "Prediction\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "120 Cost : nan \n",
      "Prediction\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "160 Cost : nan \n",
      "Prediction\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "200 Cost : nan \n",
      "Prediction\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.array([\n",
    "    [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "    [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "    [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "    [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "    [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "    [819, 823, 1198100, 816, 820.450012],\n",
    "    [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "    [809.51001, 816.659973, 1398100, 804.539978, 809.559998]\n",
    "])\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Placeholder for a tensor that will be always frd.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(201):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    if step % 40 == 0:\n",
    "        print(step, \"Cost :\", cost_val,\n",
    "             \"\\nPrediction\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측이 잘 되지않은 이유는 데이터가 **Normalized**되지 않았기 때문이다.\n",
    "\n",
    "### Normalized inputs (min-max scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost : 2.1135879 \n",
      "Prediction\n",
      " [[-0.8375394 ]\n",
      " [-1.6286453 ]\n",
      " [-1.1022344 ]\n",
      " [-0.44793546]\n",
      " [-0.832747  ]\n",
      " [-0.7305834 ]\n",
      " [-0.39647448]\n",
      " [-0.3694872 ]]\n",
      "40 Cost : 2.1074905 \n",
      "Prediction\n",
      " [[-0.83465135]\n",
      " [-1.6258268 ]\n",
      " [-1.099904  ]\n",
      " [-0.44615546]\n",
      " [-0.83062065]\n",
      " [-0.72853124]\n",
      " [-0.39512596]\n",
      " [-0.36815533]]\n",
      "80 Cost : 2.101407 \n",
      "Prediction\n",
      " [[-0.83176506]\n",
      " [-1.6230099 ]\n",
      " [-1.0975752 ]\n",
      " [-0.44437712]\n",
      " [-0.8284959 ]\n",
      " [-0.7264808 ]\n",
      " [-0.39377916]\n",
      " [-0.36682522]]\n",
      "120 Cost : 2.0953507 \n",
      "Prediction\n",
      " [[-0.82888675]\n",
      " [-1.620203  ]\n",
      " [-1.0952536 ]\n",
      " [-0.4426031 ]\n",
      " [-0.82637715]\n",
      " [-0.72443604]\n",
      " [-0.39243543]\n",
      " [-0.36549872]]\n",
      "160 Cost : 2.0893104 \n",
      "Prediction\n",
      " [[-0.8260109 ]\n",
      " [-1.6173992 ]\n",
      " [-1.0929348 ]\n",
      " [-0.44083124]\n",
      " [-0.8242608 ]\n",
      " [-0.72239375]\n",
      " [-0.39109373]\n",
      " [-0.3641746 ]]\n",
      "200 Cost : 2.0832868 \n",
      "Prediction\n",
      " [[-0.8231386 ]\n",
      " [-1.6145985 ]\n",
      " [-1.0906188 ]\n",
      " [-0.4390618 ]\n",
      " [-0.82214713]\n",
      " [-0.7203541 ]\n",
      " [-0.3897541 ]\n",
      " [-0.3628522 ]]\n"
     ]
    }
   ],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Placeholder for a tensor that will be always frd.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(201):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    if step % 40 == 0:\n",
    "        print(step, \"Cost :\", cost_val,\n",
    "             \"\\nPrediction\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 데이터를 **MinMax Scaler**에 넣어 **정규화(Normalized)**한 후<br/>\n",
    "사용하니 값이 예측되는 것을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
